{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Natural Language Processing 2 Lab02"
      ],
      "metadata": {
        "id": "X5tEoGo5a_-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-decoder model\n",
        "\n",
        "Today you will implement a pretty decent machine translation model using the transformer and then compare it to what you could have, with the same amount of training time, with a combination of RNN + Attention.\n",
        "\n",
        "###  Go through the pyTorch tutorial\n",
        "\n",
        "To start with, just follow the pyTorch [language translation with nn.Transformer and torchtext tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html).\n",
        "\n",
        "To make the code turn on Google Colab, you need to update the preinstalled version of spaCy and download the small German and English spaCy models. As pyTorch doesn't seem to maintain its tutorial with their most recent changes, you also need to install torchdata.\n",
        "```\n",
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "```\n",
        "\n",
        "As the training takes time (~20min), you can start looking at the following steps while it finishes.\n",
        "\n",
        "At training, you will encounter `TypeError: ZipperIterDataPipe instance doesn't have valid length` (pyTorch doesn't update their tutorials). A workaround can be found [here](https://github.com/pytorch/tutorials/issues/1868).\n",
        "\n",
        "### **(5 points)** Decoding functions\n",
        "\n",
        "The tutorial uses a greedy approach at decoding. Implement the following variations.\n",
        "* (2 points) A top K sampling with and without temperature.\n",
        "* (3 points) A beam search (from scratch).\n",
        "* (1 point) Qualitatively compare a few (at least 3) translation samples for each approach (even the greedy one).\n",
        "\n",
        "### **(2 points)** Compute the BLEU score of the model\n",
        "\n",
        "Use the [sacreBLEU](https://github.com/mjpost/sacreBLEU) implementation to evaluate your model and quantitatively compare the 4 implemented decoding approaches. Explain what all the output values mean (when using the `corpus_score` function).\n",
        "\n",
        "In the [python section](https://github.com/mjpost/sacrebleu#using-sacrebleu-from-python), you'll notice the library accepts more than just one possible translation as reference, but the given dataset only has one translation per sample.\n",
        "\n",
        "Using the `translate` function provided in the tutorial is pretty slow, as it translate text by text. It's recommended you modify the function to accept a list of texts as input, and batch them for translations (also **bonus point**).\n",
        "\n",
        "### **(Bonus)** Try with another language\n",
        "\n",
        "Use the [Tatoeba dataset](https://huggingface.co/datasets/tatoeba) with the language pair of your choice to train the model again. Beware that the Multi30K dataset has 29K training sample and 1K test sample, while the Tatoeba dataset only has a training set (you'll have to split it yourself) and 262K sentence pairs for their English-French data. So maybe train on a sub-sample. As a suggestion, sort the sentences per size and only use the first 30K. \n",
        "\n",
        "* Extract data from the Tatoeba dataset.\n",
        "* Train a model with it.\n",
        "* Compute the BLEU score using sacreBLEU on left-out data."
      ],
      "metadata": {
        "id": "sgIN2uYQbIix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going further\n",
        "\n",
        "If you want to understand in-depth how the transformer model works, I recommend you check [The Annotated Tranformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from HarvardNLP. This article helps you write your own transformer from scratch in pyTorch."
      ],
      "metadata": {
        "id": "sU-bC7OmbQoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "The assignment will be evaluated on the following criteria\n",
        "\n",
        "* A report answering the questions above, describing your technical choices, and analysing your results.\n",
        "* The quality of your code (modularity, efficiency, comments, coding standards).\n",
        "\n",
        "For coding standards, please respect the following guidelines\n",
        "* Use [docstring](https://www.programiz.com/python-programming/docstrings) format to describe your functions and their arguments\n",
        "* Use typing\n",
        "* Have clear and verbatim variable names (not x, x1, x2, xx, another_x, ...)\n",
        "* Make your results reproducible (force random seeds values)\n",
        "* Don't hesitate commenting in details part of the code you consider complex or hard to read\n",
        "\n",
        "Provide a `README.md` file with \n",
        "* A short description of the project\n",
        "* A description of the file/module architecture\n",
        "\n",
        "This part provides 7 points + 3 points on coding standards: naming, typing, comments, and docstring. You can earn extra points by answering the bonus questions, and by packaging your code in extra python files. At the end of the module, all project points are summed and projected on a grade between 0 and 16. The last 4 points can be earned by answering the bonus questions, and presenting a language.\n",
        "\n",
        "All projects have to be send back at `marc.von-wyl` at `epita` dot `fr` before Thursday 17th of November 2022 at midnight. Thought is is advised to send them progressively."
      ],
      "metadata": {
        "id": "lA-dF3phbN1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "nzJimhNFOh48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IUw4f1NWXQIv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gwpy"
      ],
      "metadata": {
        "id": "qKhJV7TmKQtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "KMVMPw4CXQ2C"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from typing import Tuple\n",
        "from sacrebleu.metrics import BLEU\n",
        "from sacrebleu.metrics.bleu import BLEUScore"
      ],
      "metadata": {
        "id": "J4OpP4JrxZmt"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8j-A50UXQIz"
      },
      "source": [
        "\n",
        "# Language Translation with nn.Transformer and torchtext\n",
        "\n",
        "This tutorial shows:\n",
        "    - How to train a translation model from scratch using Transformer. \n",
        "    - Use tochtext library to access  [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)_ dataset to train a German to English translation model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh3G3mTiXQI0"
      },
      "source": [
        "## Data Sourcing and Processing\n",
        "\n",
        "[torchtext library](https://pytorch.org/text/stable/)_ has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. In this example, we show how to use torchtext's inbuilt datasets, \n",
        "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
        "[Multi30k dataset from torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k)_\n",
        "that yields a pair of source-target raw sentences. \n",
        "\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torchtext==\"0.14.0\""
      ],
      "metadata": {
        "id": "O3s7aGBVYZ3I"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ilsmZ5c6XQI1"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "# pip install -U torchdata\n",
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "# python -m spacy download de_core_news_sm\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        " \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator \n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object \n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMy5jXgiXQI2"
      },
      "source": [
        "## Seq2Seq Network using Transformer\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in [“Attention is all you\n",
        "need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)_\n",
        "paper for solving machine translation tasks. \n",
        "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
        "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
        "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
        "encodings to provide position information of input tokens to the model. The second part is the \n",
        "actual [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ model. \n",
        "Finally, the output of Transformer model is passed through linear layer\n",
        "that give un-normalized probabilities for each token in the target language. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MZdhGc6TXQI4"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network \n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UPGv-jAXQI5"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent model to look into\n",
        "the future words when making predictions. We will also need masks to hide\n",
        "source and target padding tokens. Below, let's define a function that will take care of both. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "25D5wvq3XQI5"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVC_R8PlXQI6"
      },
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also \n",
        "define our loss function which is the cross-entropy loss and the optmizer used for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6vumuBtgXQI6"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYHYwEtfXQI7"
      },
      "source": [
        "## Collation\n",
        "\n",
        "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n",
        "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n",
        "defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n",
        "can be fed directly into our model.   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PyIAW2lCXQI8"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rAawLLjXQI8"
      },
      "source": [
        "Let's define training and evaluation loop that will be called for each \n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j9NxfviRXQI9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    \n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQLClOFXQI-"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lIuEnEVsXQI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffef063c-a368-4268-dd57-16c290ff560d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.344, Val loss: 4.114, Epoch time = 44.824s\n",
            "Epoch: 2, Train loss: 3.761, Val loss: 3.320, Epoch time = 41.092s\n",
            "Epoch: 3, Train loss: 3.161, Val loss: 2.894, Epoch time = 42.189s\n",
            "Epoch: 4, Train loss: 2.768, Val loss: 2.638, Epoch time = 43.626s\n",
            "Epoch: 5, Train loss: 2.480, Val loss: 2.441, Epoch time = 41.943s\n",
            "Epoch: 6, Train loss: 2.250, Val loss: 2.315, Epoch time = 42.334s\n",
            "Epoch: 7, Train loss: 2.060, Val loss: 2.201, Epoch time = 43.644s\n",
            "Epoch: 8, Train loss: 1.897, Val loss: 2.113, Epoch time = 41.912s\n",
            "Epoch: 9, Train loss: 1.754, Val loss: 2.058, Epoch time = 42.058s\n",
            "Epoch: 10, Train loss: 1.631, Val loss: 2.002, Epoch time = 41.924s\n",
            "Epoch: 11, Train loss: 1.524, Val loss: 1.975, Epoch time = 42.257s\n",
            "Epoch: 12, Train loss: 1.420, Val loss: 1.945, Epoch time = 43.132s\n",
            "Epoch: 13, Train loss: 1.333, Val loss: 1.967, Epoch time = 41.861s\n",
            "Epoch: 14, Train loss: 1.251, Val loss: 1.942, Epoch time = 42.154s\n",
            "Epoch: 15, Train loss: 1.173, Val loss: 1.928, Epoch time = 41.730s\n",
            "Epoch: 16, Train loss: 1.103, Val loss: 1.914, Epoch time = 42.327s\n",
            "Epoch: 17, Train loss: 1.039, Val loss: 1.903, Epoch time = 42.261s\n",
            "Epoch: 18, Train loss: 0.979, Val loss: 1.907, Epoch time = 42.148s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm \n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dSPtaQBEXQI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145e7c36-d941-40c9-afc1-2afd8aae4bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front of an igloo . \n"
          ]
        }
      ],
      "source": [
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5 points) Decoding functions"
      ],
      "metadata": {
        "id": "rDGJWMpWOdQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy decode (from tutorial)"
      ],
      "metadata": {
        "id": "Ldebgeo0kVM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Greedy method (from the given tutorial) is pretty easy to understand and use. In essence, it is converting the text input (to be translated) into a sequence of tokens and iterates on them, simply finding at each iteration the single word that the transformer model found most probable.\n",
        "\n",
        "Not much was needed for this function as it is from the given tutorial, but was very useful to use as a basis for the other decoding methods. The results will be analysed at the end of the process."
      ],
      "metadata": {
        "id": "21xK6lYen1nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tutorial function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model: torch.nn.Module, src: str, src_mask: torch.bool, max_len: int, start_symbol: int) -> torch.long:\n",
        "    \"\"\"\n",
        "    Tutorial function to generate output sequence using greedy algorithm\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src (str): the text to translate\n",
        "        src_mask (torch.bool): a mask that will be used to encode the src\n",
        "        max_len (int): the maximum number of iterations for the main translation loop\n",
        "        start_symbol (int): the first symbol that will be in the translation (BOS_IDX)\n",
        "\n",
        "    Returns:\n",
        "        torch.long: returns th translation in form of a torch tensor\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len - 1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(\n",
        "            DEVICE\n",
        "        )\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# Tutorial actual function to translate input sentence into target language\n",
        "def greedy_translate(model: torch.nn.Module, src_sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    Tutorial actual function to translate input sentence into target language\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src_sentence (str): the sentence to translate\n",
        "\n",
        "    Returns:\n",
        "        str: the readable text translation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX\n",
        "    ).flatten()\n",
        "    return (\n",
        "        \" \".join(\n",
        "            vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
        "        )\n",
        "        .replace(\"<bos>\", \"\")\n",
        "        .replace(\"<eos>\", \"\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "hsM3yETSkUNY"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Greedy decode (tutorial) on a classic example with variations of its parameters."
      ],
      "metadata": {
        "id": "KFYe7OXvnnWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now test our top-k to translate the following sentence:\n",
        "to_translate1 = \"Menschen stehen vor einem Iglu und Essen wird zubereitet\"\n",
        "to_translate2 = \"Motorräder werden vor einem Parkplatz abgestellt\"\n",
        "print(\"Original phrases:\")\n",
        "print(\"1 ->\", to_translate1)\n",
        "print(\"2 ->\", to_translate2)\n",
        "\n",
        "# Greedy decode translation:\n",
        "print(\"\\nTranslated phrase with greedy decode:\")\n",
        "translated_phrase1 = greedy_translate(transformer, to_translate1)\n",
        "print(\"1 ->\" + translated_phrase1)\n",
        "translated_phrase2 = greedy_translate(transformer, to_translate2)\n",
        "print(\"2 ->\" + translated_phrase2)\n",
        "\n",
        "# Proper Google translation result:\n",
        "print(\"\\nGoogle Translate phrase:\")\n",
        "print(\"1 ->\", \"People are standing in front of an igloo and food is being prepared\")\n",
        "print(\"2 ->\", \"Motorbikes are parked in front of a parking lot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwF1lzgnmBkN",
        "outputId": "7d8c70a1-9356-424a-d118-acb11e125ffc"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original phrases:\n",
            "1 -> Menschen stehen vor einem Iglu und Essen wird zubereitet\n",
            "2 -> Motorräder werden vor einem Parkplatz abgestellt\n",
            "\n",
            "Translated phrase with greedy decode:\n",
            "1 -> People stand in front of an igloo and food . \n",
            "2 -> motorcycles are being lifted by in front of a parking lot .\n",
            "\n",
            "Google Translate phrase:\n",
            "1 -> People are standing in front of an igloo and food is being prepared\n",
            "2 -> Motorbikes are parked in front of a parking lot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result analysis:\n",
        "We can see that this decoding function is providing a decent translation.\n",
        "\n",
        "For this example, a part of the original first sentence is pretty much ignored (\"is being prepared\").\n",
        "\n",
        "The second sentence also has a translation error: the motorbikes are supposed to be parked, not lifted as predicted."
      ],
      "metadata": {
        "id": "QqDE919mM_3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2 points) A top K sampling with and without temperature"
      ],
      "metadata": {
        "id": "J66hIOgFwV5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Top-K method of decoding operates the same way as the greedy decoding except for a major difference: it is selecting the k most probable translations made by the model when given a token (at each iteration) and randomly chooses one of them, based on their normalized probabilities.\n",
        "\n",
        "The k value can be adjusted to have a broader selection.\n",
        "\n",
        "The Top-K can also be implemented with a temperature, a parameter ranging from 0.0 to 1.0, used to extrapolate the existing probabilities the colder the temperature is.\n",
        "\n",
        "At temperature = 1.0, nothing will change, and close to 0.0 is will be similar to a greedy search (the most probable translation will be so maximized that it will dominate the others by far, becoming almost impossible to not be selected when extremely close to 0.0\n",
        "\n",
        "The implementation difference for the Top-K with a temperature parameter is mainly:\n",
        "- a random selection of a token translation based on the model's translation candidates probabilities after their normalization\n",
        "- a function used to apply the given temperature to this list of probabilities, with a maths formula"
      ],
      "metadata": {
        "id": "WAA5T0z4o-Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to apply temperature on given probabilities\n",
        "# Temperature ranges from 0.0 (excluded) to 1.0\n",
        "def apply_temperature(probs: List[float], temperature: float) -> List[float]:\n",
        "    \"\"\"\n",
        "    Function used to apply temperature on given probabilities\n",
        "\n",
        "    Args:\n",
        "        probs (List[float]): the probabilities we want to extrapolate by temperature\n",
        "        temperature (float): used to extrapolate existing probabilities\n",
        "\n",
        "    Returns:\n",
        "        List[float]: the temperature extrapolated probabilities\n",
        "    \"\"\"\n",
        "    exp_sum = sum([np.float_power(prob, 1.0 / temperature) for prob in probs])\n",
        "    for i in range(len(probs)):\n",
        "        probs[i] = np.float_power(probs[i], 1.0 / temperature) / exp_sum\n",
        "    return probs\n",
        "\n",
        "\n",
        "# Toolbox function used to convert tokens to readable text\n",
        "def translate_tokens(tgt_tokens: torch.Tensor) -> str:\n",
        "    \"\"\"\n",
        "    Toolbox function used to convert tokens to readable text\n",
        "\n",
        "    Args:\n",
        "        tgt_tokens (torch.Tensor): the tokens we want to translate\n",
        "\n",
        "    Returns:\n",
        "        str: the readable text translation\n",
        "    \"\"\"\n",
        "    translation = (\n",
        "        \" \".join(\n",
        "            vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
        "        )\n",
        "        .replace(\"<bos>\", \"\")\n",
        "        .replace(\"<eos>\", \"\")\n",
        "    )\n",
        "    return translation\n",
        "\n",
        "\n",
        "# Main topk loop function\n",
        "def top_k_decode(model: torch.nn.Module, src: str, src_mask: torch.bool, max_len: int, start_symbol: int, top_k: int, temperature: float) -> torch.long:\n",
        "    \"\"\"\n",
        "    Main topk loop function\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src (str): the text to translate\n",
        "        src_mask (torch.bool): a mask that will be used to encode the src\n",
        "        max_len (int): the maximum number of iterations for the main translation loop\n",
        "        start_symbol (int): the first symbol that will be in the translation (BOS_IDX)\n",
        "        top_k (int): used to select the best k translations.\n",
        "        temperature (float): used to extrapolate existing probabilities.\n",
        "\n",
        "    Returns:\n",
        "        torch.long: the translation in form of a torch tensor\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len - 1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(\n",
        "            DEVICE\n",
        "        )\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        probs, next_words = torch.topk(prob, top_k, dim=1)\n",
        "        probs_list = probs.tolist()[0]\n",
        "        next_words = next_words.tolist()[0]\n",
        "        norm_probs = [float(i) / sum(next_words) for i in next_words]\n",
        "        norm_probs = apply_temperature(norm_probs, temperature)\n",
        "        next_word = np.random.choice(next_words, 15, norm_probs)[0]\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# Main topk decoding translation function\n",
        "# This is the function to call to translate a given text\n",
        "def translate_topk(model: torch.nn.Module, src_sentence: str, top_k: int=10, temperature: float=1.0) -> str:\n",
        "    \"\"\"\n",
        "    Main topk decoding translation function\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src_sentence (str): the sentence to translate\n",
        "        top_k (int, optional): used to select the best k translations. Defaults to 10.\n",
        "        temperature (float, optional): used to extrapolate existing probabilities. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        str: the readable text translation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = top_k_decode(\n",
        "        model,\n",
        "        src,\n",
        "        src_mask,\n",
        "        max_len=num_tokens + 5,\n",
        "        start_symbol=BOS_IDX,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "    ).flatten()\n",
        "    return translate_tokens(tgt_tokens)\n"
      ],
      "metadata": {
        "id": "_xtkhhV_wNYK"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Top-K method on a classic example with variations of its parameters."
      ],
      "metadata": {
        "id": "zWyrx7QkngMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now test our top-k to translate the following sentence:\n",
        "to_translate = \"Menschen stehen vor einem Iglu und Essen wird zubereitet\"\n",
        "print(\"Original phrase:\")\n",
        "print(to_translate)\n",
        "\n",
        "# Setting up the different temperatures we will be using for testing\n",
        "temperature1 = 1.0\n",
        "temperature2 = 0.5\n",
        "temperature3 = 0.3\n",
        "temperature4 = 0.01\n",
        "\n",
        "# Setting up the random seed for reproductible results\n",
        "np.random.seed(6464)\n",
        "\n",
        "# Top-k without temperature and k = 1\n",
        "top_k = 1\n",
        "print(\"\\nTranslated phrase with k =\", top_k, \"(same as greedy decode):\")\n",
        "translated_phrase = translate_topk(transformer, to_translate, top_k)\n",
        "print(\"->\" + translated_phrase)\n",
        "\n",
        "# Top-k with a temperature of 1.0 \n",
        "top_k = 10\n",
        "print(\"\\nTranslated phrase with k =\", top_k, \"and various temperatures:\")\n",
        "print(\"Temperature =\", temperature1)\n",
        "translated_phrase = translate_topk(transformer, to_translate, top_k, temperature1)\n",
        "print(\"->\" + translated_phrase)\n",
        "# Top-k with a temperature of 0.5\n",
        "print(\"Temperature =\", temperature2)\n",
        "translated_phrase = translate_topk(transformer, to_translate, top_k, temperature2)\n",
        "print(\"->\" + translated_phrase)\n",
        "# Top-k with a temperature of 0.3\n",
        "print(\"Temperature =\", temperature3)\n",
        "translated_phrase = translate_topk(transformer, to_translate, top_k, temperature3)\n",
        "print(\"->\" + translated_phrase)\n",
        "# Top-k with a temperature of 0.01\n",
        "print(\"Temperature =\", temperature4)\n",
        "translated_phrase = translate_topk(transformer, to_translate, top_k, temperature4)\n",
        "print(\"->\" + translated_phrase)\n",
        "\n",
        "# Proper Google translation result:\n",
        "print(\"\\nGoogle Translate phrase:\")\n",
        "print(\"People are standing in front of an igloo and food is being prepared\")\n"
      ],
      "metadata": {
        "id": "P_M-kirU3Fkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524dabc6-38a3-49ac-e822-150acab2f403"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original phrase:\n",
            "Menschen stehen vor einem Iglu und Essen wird zubereitet\n",
            "\n",
            "Translated phrase with k = 1 (same as greedy decode):\n",
            "-> People stand in front of an igloo and food . \n",
            "\n",
            "Translated phrase with k = 10 and various temperatures:\n",
            "Temperature = 1.0\n",
            "-> Several persons perform before prepare food in an igloo machine is getting cooking . up\n",
            "Temperature = 0.5\n",
            "-> Groups of individuals , in Hawaiian shirts while doing preparing prepare meal and eating \n",
            "Temperature = 0.3\n",
            "-> There , some other individuals standing , before some food stand cooking on . and\n",
            "Temperature = 0.01\n",
            "-> Groups pf persons stand , including people getting meat while in the front setting watches\n",
            "\n",
            "Google Translate phrase:\n",
            "People are standing in front of an igloo and food is being prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result analysis:\n",
        "This time, the results are much less accurate. \n",
        "\n",
        "Is is difficult to determine whether the temperature has a positive effect or not.\n",
        "\n",
        "As expected, a top-k of 1 is the same as the greedy decode."
      ],
      "metadata": {
        "id": "weqRlPbRPffO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3 points) A beam search (from scratch)."
      ],
      "metadata": {
        "id": "Id_4Y1EnOpKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Beam Search method has a great difference with the 3 previous ones, and is considered as one of the most powerful decoding method.\n",
        "\n",
        "The reason for that is that is runs a number (beam width) of parallel translations that will each be updated at each iteration (choosing another number of \"beam\" possible translations for them) and keeping only the \"beam\" best translations.\n",
        "\n",
        "This allows the method to reconsider it's best translations at each iteration by taking the context into account, for a single word can change the meaning of the sentence we are translating at any moment.\n",
        "\n",
        "This implementation was focused on:\n",
        "- making a function sorting the \"beam\" best new translations from a given list of translations (like a top-k)\n",
        "- getting the \"beam\" best possible translations from a given token, a process applied to each \"beam\" previous (ongoing) translations, which are in essence the best current candidates at a given moment "
      ],
      "metadata": {
        "id": "bsPJ8M4wrYa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the k best ys from a given list of ys and their probabilities\n",
        "# This function is used to isolate the current \"beam width\" best ys translations\n",
        "def get_k_best_ys(ys_list: List, probs: List[float], k: int) -> Tuple[List, List[float]]:\n",
        "    \"\"\"\n",
        "    Finds the k best ys from a given list of ys and their probabilities\n",
        "\n",
        "    Args:\n",
        "        ys_list (List): given list of translations as torch tensors\n",
        "        probs (List[float]): given list of probabilities of the ys_list (translations)\n",
        "        k (int): the number of best translations we want\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List, List[float]]: the best translation candidates, their probabilities\n",
        "    \"\"\"\n",
        "    best_ys = []\n",
        "    best_probs = []\n",
        "    for i in range(len(probs)):\n",
        "        if len(best_ys) < k:\n",
        "            best_ys.append(ys_list[i])\n",
        "            best_probs.append(probs[i])\n",
        "        else:\n",
        "            min_index = best_probs.index(min(best_probs))\n",
        "            best_ys[min_index] = ys_list[i]\n",
        "            best_probs[min_index] = probs[i]\n",
        "    return best_ys, best_probs\n",
        "\n",
        "\n",
        "# Return the top-k best next translations (given ys + new word) by decoding\n",
        "# k is the beam_width in this situation\n",
        "def get_k_best_translation_candidates(ys: torch.long, k: int, memory: torch.tensor, model: torch.nn.Module, src: str) -> Tuple[List, List[float], List[int]]:\n",
        "    \"\"\"\n",
        "    Used to get the top-k best next translations (given ys + new word) by decoding\n",
        "\n",
        "    Args:\n",
        "        ys (torch.long): the current translation as a torch tensor\n",
        "        k (int): the number of best translations we want\n",
        "        memory (torch.tensor): the memory of the translation, used to avoid word by word translation (without context)\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src (str): the text to translate\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List, List[float], List[int]]: the translation current word candidates, their probabilities, the associated words\n",
        "    \"\"\"\n",
        "    memory = memory.to(DEVICE)\n",
        "    tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "    out = model.decode(ys, memory, tgt_mask)\n",
        "    out = out.transpose(0, 1)\n",
        "    prob = model.generator(out[:, -1])\n",
        "    probs, next_words = torch.topk(prob, k, dim=1)\n",
        "    ys_candidates_probas = probs.tolist()[0]\n",
        "    next_words = next_words.tolist()[0]\n",
        "\n",
        "    ys_candidates = []\n",
        "    for i in range(len(next_words)):\n",
        "        next_word = next_words[i]\n",
        "        new_ys = torch.cat(\n",
        "            [ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0\n",
        "        )\n",
        "        ys_candidates.append(new_ys)\n",
        "    return ys_candidates, ys_candidates_probas, next_words\n",
        "\n",
        "\n",
        "# Main beam search loop function\n",
        "def beam_search_decode(model: torch.nn.Module, src: str, src_mask: torch.bool, max_len: int, start_symbol: int, beam_width: int) -> torch.long:\n",
        "    \"\"\"\n",
        "    Function used to generate output sequence using a beam search method\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src (str): the text to translate\n",
        "        src_mask (torch.bool): a mask that will be used to encode the src\n",
        "        max_len (int): the maximum number of iterations for the main translation loop\n",
        "        start_symbol (int): the first symbol that will be in the translation (BOS_IDX)\n",
        "        beam_width (int): the width of the beam search: number of parallel translations.\n",
        "\n",
        "    Returns:\n",
        "        torch.long: the best found translation in form of a torch tensor\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    beams_ys, beams_ys_probs, beam_words = get_k_best_translation_candidates(\n",
        "        ys, beam_width, memory, model, src\n",
        "    )\n",
        "\n",
        "    for i in range(1, max_len - 1):\n",
        "        ys_candidates = []\n",
        "        ys_candidates_probas = []\n",
        "        next_beam_words = []\n",
        "        for beam_ys in beams_ys:\n",
        "            new_ys, new_probas, tmp_beam_words = get_k_best_translation_candidates(\n",
        "                beam_ys, beam_width, memory, model, src\n",
        "            )\n",
        "            ys_candidates += new_ys\n",
        "            ys_candidates_probas += new_probas\n",
        "            next_beam_words += beam_words\n",
        "\n",
        "        beams_ys, beams_ys_probs = get_k_best_ys(\n",
        "            ys_candidates, ys_candidates_probas, beam_width\n",
        "        )\n",
        "\n",
        "    max_index = beams_ys_probs.index(max(beams_ys_probs))\n",
        "    return beams_ys[max_index]\n",
        "\n",
        "\n",
        "# Toolbox function used to convert tokens to text while cutting at the first EOS\n",
        "def translate_tokens_EOS(tgt_tokens: torch.Tensor) -> str:\n",
        "    \"\"\"\n",
        "    Toolbox function used to convert tokens to text while cutting at the first EOS\n",
        "\n",
        "    Args:\n",
        "        tgt_tokens (torch.Tensor): the tokens we want to translate\n",
        "\n",
        "    Returns:\n",
        "        str: the readable text translation\n",
        "    \"\"\"\n",
        "    translation = \" \".join(\n",
        "        vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
        "    )\n",
        "    eos_index = translation.index(\"<eos>\")\n",
        "    translation = translation[:eos_index].replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    return translation\n",
        "\n",
        "\n",
        "# Main beam search translation function\n",
        "# This is the function to call to translate a given text\n",
        "def translate_beamsearch(model: torch.nn.Module, src_sentence: str, beam_width: int=10) -> str:\n",
        "    \"\"\"\n",
        "    Main beam search translation function\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): the transformer used to predict a translation\n",
        "        src_sentence (str): the sentence to translate\n",
        "        beam_width (int, optional): the width of the beam search: number of parallel translations. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        str: the readable text translation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = beam_search_decode(\n",
        "        model,\n",
        "        src,\n",
        "        src_mask,\n",
        "        max_len=num_tokens + 5,\n",
        "        start_symbol=BOS_IDX,\n",
        "        beam_width=beam_width,\n",
        "    ).flatten()\n",
        "    return translate_tokens_EOS(tgt_tokens)\n"
      ],
      "metadata": {
        "id": "L2b16-BLvDga"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Beam Search method on a classic example with variations of its parameters."
      ],
      "metadata": {
        "id": "FUXcgoROnSXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now test our Beam Search to translate the following sentence:\n",
        "to_translate = \"Menschen stehen vor einem Iglu und Essen wird zubereitet\"\n",
        "print(\"Original phrase:\")\n",
        "print(to_translate)\n",
        "\n",
        "beam_width = 1\n",
        "# Beam Search with a beam width of 1: \n",
        "print(\"\\nTranslated phrase with Beam Width =\", beam_width, \":\")\n",
        "translated_phrase = translate_beamsearch(transformer, to_translate, beam_width)\n",
        "print(\"->\" + translated_phrase)\n",
        "\n",
        "beam_width = 5\n",
        "# Beam Search with a beam width of 3:\n",
        "print(\"\\nTranslated phrase with Beam Width =\", beam_width, \":\")\n",
        "translated_phrase = translate_beamsearch(transformer, to_translate, beam_width)\n",
        "print(\"->\" + translated_phrase)\n",
        "\n",
        "beam_width = 10\n",
        "# Beam Search with a beam width of 5:\n",
        "print(\"\\nTranslated phrase with Beam Width =\", beam_width, \":\")\n",
        "translated_phrase = translate_beamsearch(transformer, to_translate, beam_width)\n",
        "print(\"->\" + translated_phrase)\n",
        "\n",
        "# Proper Google translation result:\n",
        "print(\"\\nGoogle Translate phrase:\")\n",
        "print(\"People are standing in front of an igloo and food is being prepared\")\n"
      ],
      "metadata": {
        "id": "ccbVljl-zCg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38292ba0-b51a-4853-d89c-a21e2861c481"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original phrase:\n",
            "Menschen stehen vor einem Iglu und Essen wird zubereitet\n",
            "\n",
            "Translated phrase with Beam Width = 1 :\n",
            "-> People stand in front of an igloo and food . \n",
            "\n",
            "Translated phrase with Beam Width = 5 :\n",
            "-> Several people stand ready in front of an igloo \n",
            "\n",
            "Translated phrase with Beam Width = 10 :\n",
            "-> People stand in front of an igloo and cooking . \n",
            "\n",
            "Google Translate phrase:\n",
            "People are standing in front of an igloo and food is being prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result analysis:\n",
        "The Beam Search decoding seem much more efficient and accurate than the top-k. \n",
        "\n",
        "The result is even closer to the actual translation when the beam width is pretty large (10)."
      ],
      "metadata": {
        "id": "8WN2vnH_RnUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SacreBLEU"
      ],
      "metadata": {
        "id": "qZprp_NkiSGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU (bilingual evaluation understudy) scoring system, as a function, takes a list of translation hypotheses as input and compares them to the actual translation to give a score of accuracy.\n",
        "\n",
        "The higher the score, the better the translation.\n",
        "\n",
        "The implementation here is pretty simple, consisting only in a short sacreBleu scoring function (sacrebleu_score) and a pretty print function to make the result proper and readable.\n",
        "\n",
        "This step is concluded by an analysis of the results, comparing the function's scores and the actual differences from a human perspective."
      ],
      "metadata": {
        "id": "0Od44MMXlRhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that computes and returns a BLEU score by comparing the given predictions (hypotheses translations) with the references (actual translations)\n",
        "def sacrebleu_score(hypotheses: List[str], references: List[str]) -> BLEUScore:\n",
        "    \"\"\"\n",
        "    Function that computes and returns a BLEU score by comparing the given predictions (hypotheses translations) with the references (actual translations)\n",
        "\n",
        "    Args:\n",
        "        hypotheses (List[str]): the given predictions (hypotheses translations)\n",
        "        references (List[str]): the references (actual translations)\n",
        "\n",
        "    Returns:\n",
        "        BLEUScore: the computed BLEU score\n",
        "    \"\"\"\n",
        "    bleu = BLEU()\n",
        "    result = bleu.corpus_score(hypotheses, references)\n",
        "    return result\n",
        "\n",
        "\n",
        "# Function used to pretty print the BLEU score of a decoding function\n",
        "def pretty_print_bleu_score(hypotheses: List[str], references: List[str], decode_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Function used to pretty print the BLEU score of a decoding function\n",
        "\n",
        "    Args:\n",
        "        hypotheses (List[str]): the given predictions (hypotheses translations) that we will print\n",
        "        references (List[str]): the references (actual translations) used to compute the BLEU score\n",
        "        decode_name (str): the name of the decoding function to pretty print the results as a readable output\n",
        "    \"\"\"\n",
        "    BLEU_score = sacrebleu_score(hypotheses, references)\n",
        "    print(\"\\n->\", decode_name, \"hypotheses:\")\n",
        "    print(hypotheses)\n",
        "    print(\"    + Score BLEU =\", BLEU_score.score)\n",
        "    print(\"    + BLEU score details:\", BLEU_score)\n"
      ],
      "metadata": {
        "id": "mU76rGSir4Bt"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences that we want to translate to find the BLEU score of the 4 decoding approaches\n",
        "to_be_translated = [\"Menschen stehen vor einem Iglu und Essen wird zubereitet\",\n",
        "                    \"Motorräder werden vor einem Parkplatz abgestellt\",\n",
        "                    \"Autos sind auf der Autobahn\",\n",
        "                    \"Der Sänger beginnt für das Publikum zu singen\"]\n",
        "# The Google Translate references (actual translations) of the previous sentences, used as references for the BLEU score\n",
        "references = [\"People are standing in front of an igloo and food is being prepared\",\n",
        "              \"Motorbikes are parked in front of a parking lot\",\n",
        "              \"Cars are on the highway\",\n",
        "              \"The singer begins to sing for the audience\"]\n",
        "\n",
        "# Printing the sentences we want to translate and their translations for result clarity\n",
        "print(\"\\nSentences to translate:\")\n",
        "print(to_be_translated)\n",
        "print(\"\\nREFERENCES: Google Translation:\")\n",
        "print(references)\n",
        "\n",
        "# \n",
        "hypotheses_greedy = [greedy_translate(transformer, sentence) for sentence in to_be_translated]\n",
        "pretty_print_bleu_score(hypotheses_greedy, references, \"Greedy\")\n",
        "\n",
        "hypotheses_topk = [translate_topk(transformer, sentence, top_k=5, temperature=1.0) for sentence in to_be_translated]\n",
        "pretty_print_bleu_score(hypotheses_topk, references, \"Top-k without temperature\")\n",
        "\n",
        "hypotheses_topk_temperature = [translate_topk(transformer, sentence, top_k=5, temperature=0.5) for sentence in to_be_translated]\n",
        "pretty_print_bleu_score(hypotheses_topk_temperature, references, \"Top-k with temperature\")\n",
        "\n",
        "hypotheses_beamsearch = [translate_beamsearch(transformer, sentence, beam_width=10) for sentence in to_be_translated]\n",
        "pretty_print_bleu_score(hypotheses_beamsearch, references, \"Beam Search\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvXJmmm2tFOA",
        "outputId": "bf5f5b33-3cb4-4c7d-9d84-369a145c0f22"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentences to translate:\n",
            "['Menschen stehen vor einem Iglu und Essen wird zubereitet', 'Motorräder werden vor einem Parkplatz abgestellt', 'Autos sind auf der Autobahn', 'Der Sänger beginnt für das Publikum zu singen']\n",
            "\n",
            "REFERENCES: Google Translation:\n",
            "['People are standing in front of an igloo and food is being prepared', 'Motorbikes are parked in front of a parking lot', 'Cars are on the highway', 'The singer begins to sing for the audience']\n",
            "\n",
            "-> Greedy hypotheses:\n",
            "[' People stand in front of an igloo and food . ', ' motorcycles are being lifted by in front of a parking lot .', ' cars on the highway . ', ' The singer is trying to sing the audience . ']\n",
            "    + Score BLEU = 1.1919267551142738\n",
            "    + BLEU score details: BLEU = 1.19 2.8/1.6/0.9/0.5 (BP = 1.000 ratio = 9.000 hyp_len = 36 ref_len = 4)\n",
            "\n",
            "-> Top-k without temperature hypotheses:\n",
            "[' Some persons are in the igloo outside in a cemetery cooking . . outside ', ' motorbikes being played together outside in a crowded area car ', ' cars in this road . car on the bikes track equipment', ' While singer attempt after singing at to play . \" . \" players \\'s']\n",
            "    + Score BLEU = 0.8267162283334469\n",
            "    + BLEU score details: BLEU = 0.83 2.0/1.1/0.6/0.3 (BP = 1.000 ratio = 12.250 hyp_len = 49 ref_len = 4)\n",
            "\n",
            "-> Top-k with temperature hypotheses:\n",
            "[' Some individuals in traditional igloo outside , cooking equipment ready for sale . . area', ' bicycles getting ready with bicycles in a crowded room car outdoors car', ' cars on the streets of their riders in helmets . car', \" The singer gets to sing for the photo 's audience . . \"]\n",
            "    + Score BLEU = 0.8077274192152345\n",
            "    + BLEU score details: BLEU = 0.81 2.0/1.1/0.6/0.3 (BP = 1.000 ratio = 12.500 hyp_len = 50 ref_len = 4)\n",
            "\n",
            "-> Beam Search hypotheses:\n",
            "[' People stand in front of an igloo and cooking . ', ' There is bicycles in front by in a parking lot . ', ' There are cars on the highway . ', ' Hockey players attempt to sing for the audience . ']\n",
            "    + Score BLEU = 1.1526344320031532\n",
            "    + BLEU score details: BLEU = 1.15 2.7/1.5/0.9/0.5 (BP = 1.000 ratio = 9.250 hyp_len = 37 ref_len = 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result analysis:\n",
        "We can see that the BLEU score of the greedy and Beam Search are pretty close (both close to 1.20), while the top-k predictions somewhat lacking (around 0.80).\n",
        "\n",
        "However, when we actually read the translations as humans, the Beam Search method makes more contextual sense as compared to the greedy one, and is more easily understandable, even if the wording is somewhat different from the original sentences.\n",
        "\n",
        "The BLEU score is a good indicator, but it seems a human reader could interpret the results differently."
      ],
      "metadata": {
        "id": "68chMnYEanCp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmb0zD6bXQI_"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Attention is all you need paper.\n",
        "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nzJimhNFOh48",
        "Xh3G3mTiXQI0",
        "xMy5jXgiXQI2",
        "zYHYwEtfXQI7"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}