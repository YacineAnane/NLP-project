{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fde6883",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "The IMDB sentiment dataset is a collection of 50K movie reviews, annotated as positive or negative, and split in two sets of equal size: a training and a test set. Both set have an equal number of positive and negative review. The dataset is available on several libraries, but we ask that you use the HuggingFace [datasets](https://huggingface.co/datasets/imdb) version. Follow their [tutorial](https://huggingface.co/docs/datasets/load_hub) on how to use the library for more details.\n",
    "\n",
    "Download and look at the dataset, and answer the following questions.\n",
    "1. How many splits does the dataset has? (1 point)\n",
    "2. How big are these splits? (1 point)\n",
    "3. What is the proportion of each class on the supervised splits? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3a7873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}\n",
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /home/yacine/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 84.1M/84.1M [04:21<00:00, 322kB/s]\n",
      "                                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/yacine/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1014.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset\n",
    "\n",
    "database_name = \"imdb\"\n",
    "ds_builder = load_dataset_builder(database_name)\n",
    "print(ds_builder.info.description)\n",
    "print(ds_builder.info.features)\n",
    "\n",
    "dataset = load_dataset(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004e5408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split names ['train', 'test', 'unsupervised']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "print(\"Split names\", get_dataset_split_names(database_name))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656680a",
   "metadata": {},
   "source": [
    "We can see that this database has 3 splits. The \"train\" and \"test\" splits have 25000 rows each and the unsupervised split has 50000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efdd18ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = dataset[\"train\"].to_pandas()\n",
    "test = dataset[\"test\"].to_pandas()\n",
    "supervised = pd.concat([train, test])\n",
    "\n",
    "test.value_counts\n",
    "train.value_counts()\n",
    "supervised[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e148d6",
   "metadata": {},
   "source": [
    "We can see that there are as many positive as negative reviews in the supervised split.Indeed, each class has 25000 occurrences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_3.10.4",
   "language": "python",
   "name": "ml_3.10.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
