{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fde6883",
   "metadata": {
    "id": "5fde6883"
   },
   "source": [
    "# The dataset\n",
    "\n",
    "The IMDB sentiment dataset is a collection of 50K movie reviews, annotated as positive or negative, and split in two sets of equal size: a training and a test set. Both set have an equal number of positive and negative review. The dataset is available on several libraries, but we ask that you use the HuggingFace [datasets](https://huggingface.co/datasets/imdb) version. Follow their [tutorial](https://huggingface.co/docs/datasets/load_hub) on how to use the library for more details.\n",
    "\n",
    "Download and look at the dataset, and answer the following questions.\n",
    "1. How many splits does the dataset has? (1 point)\n",
    "2. How big are these splits? (1 point)\n",
    "3. What is the proportion of each class on the supervised splits? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ssy4mPR54HYx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ssy4mPR54HYx",
    "outputId": "fe251041-d1ac-422d-8383-b65fedc0aa87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/yacine/.local/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (1.23.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: aiohttp in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: multiprocess in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: packaging in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/yacine/.local/lib/python3.8/site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/yacine/.local/lib/python3.8/site-packages (from responses<0.19->datasets) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/yacine/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yacine/.local/lib/python3.8/site-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yacine/.local/lib/python3.8/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/yacine/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: filelock in /home/yacine/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yacine/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yacine/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/yacine/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560f1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy) (4.64.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[K     |████████████████████████████████| 461 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy) (1.23.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: jinja2 in /home/yacine/.local/lib/python3.8/site-packages (from spacy) (3.0.3)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (817 kB)\n",
      "\u001b[K     |████████████████████████████████| 817 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.22.0)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (45.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/yacine/.local/lib/python3.8/site-packages (from spacy) (1.9.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 5.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/yacine/.local/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting blis<0.10.0,>=0.7.8\n",
      "  Downloading blis-0.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.1-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yacine/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/yacine/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Installing collected packages: click, typer, catalogue, cymem, langcodes, srsly, murmurhash, blis, wasabi, preshed, confection, thinc, spacy-loggers, smart-open, pathy, spacy-legacy, spacy\n",
      "Successfully installed blis-0.9.1 catalogue-2.0.8 click-8.1.3 confection-0.0.1 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.1 typer-0.4.2 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "-fnTKz_P3ekZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fnTKz_P3ekZ",
    "outputId": "b4e0db4f-bd3e-47fb-f40f-f7b422187897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-15 10:59:44.611559: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-09-15 10:59:49.556915: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-15 10:59:49.556975: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (yacine-ROG-Strix-G533ZW-G533ZW): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/yacine/.local/lib/python3.8/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: jinja2 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.22.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (45.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yacine/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/yacine/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/yacine/.local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in /home/yacine/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/yacine/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yacine/.local/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yacine/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yacine/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf3a7873",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "4ad766b38ffe4007b334e0f152a03584",
      "f802aac9442d460987498a692a417875",
      "404804524c8244babb9ec733793b4862",
      "20c84f793d7c421ab279e43d0dc90979",
      "5ecc4acd0ae742d08a2cd60961611460",
      "4ffaae2a32f9405fb72dcb60bbec0796",
      "36dae29f9bdf4fd19112ae447f108d46",
      "beeb81c07ee0433da0ff5be34562c12f",
      "4a5bc64e1db24a71b7c38f5866355f51",
      "caf8af8b5a7245feb18b49ed18b89fc7",
      "3d291fe4a8844a9f9ffd249d02381a0a"
     ]
    },
    "id": "bf3a7873",
    "outputId": "ce9ccd00-a985-4403-b228-61b0ba3eb4d7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a27465e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "database_name = \"imdb\"\n",
    "ds_builder = load_dataset_builder(database_name)\n",
    "print(ds_builder.info.description)\n",
    "print(ds_builder.info.features)\n",
    "\n",
    "dataset = load_dataset(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004e5408",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "004e5408",
    "outputId": "83c90b25-c518-40d8-e0cd-71a2db5f8282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split names ['train', 'test', 'unsupervised']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "print(\"Split names\", get_dataset_split_names(database_name))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656680a",
   "metadata": {
    "id": "c656680a"
   },
   "source": [
    "We can see that this database has 3 splits. The \"train\" and \"test\" splits have 25000 rows each and the unsupervised split has 50000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdd18ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efdd18ae",
    "outputId": "1f1f49a2-a677-4057-d4b1-0da702bba31e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test values count : 25000\n",
      "Train values count : 25000\n"
     ]
    }
   ],
   "source": [
    "# To start we are going to split our datasets into 3 differents datasets\n",
    "train = dataset[\"train\"].to_pandas()\n",
    "test = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# Then we will have a look on the \n",
    "print(\"Test values count : {0}\".format(len(test)))\n",
    "print(\"Train values count : {0}\".format(len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e148d6",
   "metadata": {
    "id": "31e148d6"
   },
   "source": [
    "We can see that there are as many positive as negative reviews in the supervised split.Indeed, each class has 25000 occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd424d",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier **(9 points)**\n",
    "\n",
    "Implement your own naive Bayes classifier (the pseudo code can be found in the slides or the [book reference](https://web.stanford.edu/~jurafsky/slp3/)) or use [one provided by scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) combined with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Go through the following steps.\n",
    "1. (2 points) Take a look at the data and create an adapted preprocessing function which at least:\n",
    "   1. Lower case the text.\n",
    "   2. Remove punctuation (you can use `from string import punctuation` to ease your work).\n",
    "2. (4 points) Implement and train a naive Bayes classifier on the training data. Either:\n",
    "   * Code your own classifier following the algorithm given in class.\n",
    "   * Or use a scikit-learn [Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier. (Recommended)\n",
    "3. (1 point) Report the accuracy on both training and test set.\n",
    "4. (1 point) Why is accuracy a sufficient measure of evaluation here?\n",
    "5. **\\[Bonus\\]** What are the top 10 most important words (features) for each class? (bonus points)\n",
    "   1. Look at the words with the highest likelihood in each class (if you use scikit-learn, you want to check `feature_log_prob_`).\n",
    "   2. Remove stopwords (see [NLTK stopwords corpus](https://pythonspot.com/nltk-stop-words/)) and check again.\n",
    "6. Take at least 2 wrongly classified example from the test set and try explaining why the model failed. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b17630",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yvDB-0XxTVB0",
   "metadata": {
    "id": "yvDB-0XxTVB0"
   },
   "source": [
    "Let's now have a look on our dataframe and our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "BQb-IzFaTUd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BQb-IzFaTUd9",
    "outputId": "e6b06f65-b48d-4b2c-adb3-4df0108398b2",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ceefd5",
   "metadata": {},
   "source": [
    "Let's Lower case the text and remove the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0504456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility_functions\n",
    "\n",
    "# Replace uppercase with lowercases + remove punctuation\n",
    "train = utility_functions.preprocess_df(train)\n",
    "test = utility_functions.preprocess_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53626e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i rented i am curiousyellow from my video stor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am curious yellow is a risible and pretentio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this film was probably inspired by godards mas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh brotherafter hearing about this ridiculous ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i rented i am curiousyellow from my video stor...      0\n",
       "1  i am curious yellow is a risible and pretentio...      0\n",
       "2  if only to avoid making this type of film in t...      0\n",
       "3  this film was probably inspired by godards mas...      0\n",
       "4  oh brotherafter hearing about this ridiculous ...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jBXSo0F7GnfG",
   "metadata": {
    "id": "jBXSo0F7GnfG"
   },
   "source": [
    "Now let's create a Naive Bayes classifier for our dataset.\n",
    "\n",
    "To do that, we will use a Pipeline that will have 2 parameters : \n",
    "\n",
    "- One CountVectorizer to process our data\n",
    "\n",
    "- Our Naive Bayes which will be a MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2lvUcZ2T6Cgy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lvUcZ2T6Cgy",
    "outputId": "ad57e580-58de-4404-86b9-92b6deeb1c69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Vect', CountVectorizer()), ('Mnb', MultinomialNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our pipeline \n",
    "pipeline = Pipeline([('Vect', CountVectorizer()), ('Mnb', MultinomialNB())])\n",
    "\n",
    "# Fitting our pipeline on the train data (with the lables as we are doing supervised learning)\n",
    "pipeline.fit(train['text'], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "IXafLPvZVBMp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXafLPvZVBMp",
    "outputId": "b935f6c1-bb4b-4600-eb65-fcb7d5eb1537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1-score 0.8051338904997442\n",
      "test accuracy 0.8172\n",
      "train accuracy 0.91284\n"
     ]
    }
   ],
   "source": [
    "# Predicting the results of the test data \n",
    "predictions = pipeline.predict(test['text'])\n",
    "\n",
    "# Computing the F1_score to have a look on the accuracy of our model \n",
    "print(\"test f1-score\", f1_score(test['label'], predictions))\n",
    "\n",
    "print(\"test accuracy\", pipeline.score(test['text'], test['label']))\n",
    "print(\"train accuracy\", pipeline.score(train['text'], train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "VTkaWOs3Z0bw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTkaWOs3Z0bw",
    "outputId": "ad310af3-fd94-49fa-da0d-0720b458e3b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same values : 0        20430\n",
      "label    20430\n",
      "dtype: int64\n",
      "different values : 0        4570\n",
      "label    4570\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a new dataset to have another look on the results of our prediction\n",
    "test_df = pd.concat([pd.Series(predictions), test['label']], axis=1)\n",
    "\n",
    "print(\"same values :\", test_df[test_df[0] == test_df['label']].count())\n",
    "print(\"different values : {0}\".format(test_df[test_df[0] != test_df['label']].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AlAjrtW-EMoM",
   "metadata": {
    "id": "AlAjrtW-EMoM"
   },
   "source": [
    "Now let's add some processing to our data with two possible choices :\n",
    "\n",
    "- Stemming\n",
    "\n",
    "- Lemmatization \n",
    "\n",
    "Let's have a look on the results of these two before choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A2cH-UDiEsVf",
   "metadata": {
    "id": "A2cH-UDiEsVf"
   },
   "source": [
    "Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "GAj1wH_27YD1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAj1wH_27YD1",
    "outputId": "e7c0e8f4-556d-4b5c-a6b9-7f7789588570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yacine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7966195740321823"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to download a package for word tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "pipeline_stemmer = Pipeline([('Vect', CountVectorizer(analyzer=stemmed_words)), ('Mnb', MultinomialNB())])\n",
    "\n",
    "pipeline_stemmer.fit(train['text'], train['label'])\n",
    "\n",
    "# Predicting the results of the test data \n",
    "predictions_stemmer = pipeline_stemmer.predict(test['text'])\n",
    "\n",
    "# Computing the F1_score to have a look on the accuracy of our model \n",
    "f1_score(test['label'], predictions_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GfCEzd-2KICJ",
   "metadata": {
    "id": "GfCEzd-2KICJ"
   },
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "p0MeqKg0KJJW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0MeqKg0KJJW",
    "outputId": "49814b44-0ac0-4b5a-b4c1-9896b2928524"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yacine/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/yacine/nltk_data...\n",
      "/home/yacine/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8088703853179829"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")\n",
    "re_word = re.compile(r\"^\\w+$\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "pipeline_lemmatizer = Pipeline([('Vect', CountVectorizer(tokenizer=tokenize, stop_words='english')), ('Mnb', MultinomialNB())])\n",
    "\n",
    "pipeline_lemmatizer.fit(train['text'], train['label'])\n",
    "\n",
    "# Predicting the results of the test data \n",
    "predictions_lemmatizer = pipeline_lemmatizer.predict(test['text'])\n",
    "\n",
    "# Computing the F1_score to have a look on the accuracy of our model \n",
    "f1_score(test['label'], predictions_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e00bc460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer errors:\n",
      "im sorry but star wars episode 1 did not do any justice to natalie portmans talent and undeniable cuteness she was entirely underused as queen amidala and when she was used her makeup was frighteningly terrible for anywhere but here she sheds her godawful makeup and she acts normally and not only can she act good she looks good doing it im a bit older than she shes only 18 and i have little or no chance of meeting her but hey a guy is allowed to dream rightbr br even though susan sarandon does take a good turn in this movie the film belongs entirely to portman ive been a watcher of portmans since beautiful girls where she was younger but just as cute theres big things for her in the future  i can see it \n",
      "\n",
      "Lemmatized text:\n",
      "['im', 'sorry', 'but', 'star', 'war', 'episode', '1', 'did', 'not', 'do', 'any', 'justice', 'to', 'natalie', 'portmans', 'talent', 'and', 'undeniable', 'cuteness', 'she', 'wa', 'entirely', 'underused', 'a', 'queen', 'amidala', 'and', 'when', 'she', 'wa', 'used', 'her', 'makeup', 'wa', 'frighteningly', 'terrible', 'for', 'anywhere', 'but', 'here', 'she', 'shed', 'her', 'godawful', 'makeup', 'and', 'she', 'act', 'normally', 'and', 'not', 'only', 'can', 'she', 'act', 'good', 'she', 'look', 'good', 'doing', 'it', 'im', 'a', 'bit', 'older', 'than', 'she', 'shes', 'only', '18', 'and', 'i', 'have', 'little', 'or', 'no', 'chance', 'of', 'meeting', 'her', 'but', 'hey', 'a', 'guy', 'is', 'allowed', 'to', 'dream', 'rightbr', 'br', 'even', 'though', 'susan', 'sarandon', 'doe', 'take', 'a', 'good', 'turn', 'in', 'this', 'movie', 'the', 'film', 'belongs', 'entirely', 'to', 'portman', 'ive', 'been', 'a', 'watcher', 'of', 'portmans', 'since', 'beautiful', 'girl', 'where', 'she', 'wa', 'younger', 'but', 'just', 'a', 'cute', 'there', 'big', 'thing', 'for', 'her', 'in', 'the', 'future', 'i', 'can', 'see', 'it'] \n",
      "\n",
      "um hello rainbow brite the name alone is hard to take it seriously like she could be the cousin of strawberry shortcake but when youre a kid this is definitely serious stuffbr br so theres this vile snotty spoiled girl and she wants rainbow brites belt amongst other things ie the light of the whole universe and rainbow brite and her friend cris are bound and determined to stop her as i remember murky and lurky had a minor role in this production maybe they wanted too much moneybr br so anyway snotty evil girl has a powerful jewel and she channels its power to take rainbows belt imagine but somehow rainbow gets her belt back and reenergizes it with star sprinkles and kicks the bad girls butt with the help of cris and his prism bracelet and they also save the whole universe in the processbr br so good triumphs over evil niceness triumphs over rudeness and rainbow brite and the color kids are once again safe to spread color and joy for all mankind \n",
      "\n",
      "Lemmatized text:\n",
      "['um', 'hello', 'rainbow', 'brite', 'the', 'name', 'alone', 'is', 'hard', 'to', 'take', 'it', 'seriously', 'like', 'she', 'could', 'be', 'the', 'cousin', 'of', 'strawberry', 'shortcake', 'but', 'when', 'youre', 'a', 'kid', 'this', 'is', 'definitely', 'serious', 'stuffbr', 'br', 'so', 'there', 'this', 'vile', 'snotty', 'spoiled', 'girl', 'and', 'she', 'want', 'rainbow', 'brites', 'belt', 'amongst', 'other', 'thing', 'ie', 'the', 'light', 'of', 'the', 'whole', 'universe', 'and', 'rainbow', 'brite', 'and', 'her', 'friend', 'cris', 'are', 'bound', 'and', 'determined', 'to', 'stop', 'her', 'a', 'i', 'remember', 'murky', 'and', 'lurky', 'had', 'a', 'minor', 'role', 'in', 'this', 'production', 'maybe', 'they', 'wanted', 'too', 'much', 'moneybr', 'br', 'so', 'anyway', 'snotty', 'evil', 'girl', 'ha', 'a', 'powerful', 'jewel', 'and', 'she', 'channel', 'it', 'power', 'to', 'take', 'rainbow', 'belt', 'imagine', 'but', 'somehow', 'rainbow', 'get', 'her', 'belt', 'back', 'and', 'reenergizes', 'it', 'with', 'star', 'sprinkle', 'and', 'kick', 'the', 'bad', 'girl', 'butt', 'with', 'the', 'help', 'of', 'cris', 'and', 'his', 'prism', 'bracelet', 'and', 'they', 'also', 'save', 'the', 'whole', 'universe', 'in', 'the', 'processbr', 'br', 'so', 'good', 'triumph', 'over', 'evil', 'niceness', 'triumph', 'over', 'rudeness', 'and', 'rainbow', 'brite', 'and', 'the', 'color', 'kid', 'are', 'once', 'again', 'safe', 'to', 'spread', 'color', 'and', 'joy', 'for', 'all', 'mankind'] \n",
      "\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "\n",
      "Stemmer errors:\n",
      "leon errol handles his double role of uncle matt lindsay and lord basil epping superbly but i have trouble liking the mexican spitfire series because they all are contrived to produce mistaken identities and these are telegraphed way in advance errol is funny as the stuffy lord epping but i would have preferred a lot more wit and much less repetition br br  \n",
      "\n",
      "Stemmed text:\n",
      "['leon', 'errol', 'handl', 'his', 'doubl', 'role', 'of', 'uncl', 'matt', 'lindsay', 'and', 'lord', 'basil', 'ep', 'superbl', 'but', 'have', 'troubl', 'like', 'the', 'mexican', 'spitfir', 'seri', 'becaus', 'they', 'all', 'are', 'contriv', 'to', 'produc', 'mistaken', 'ident', 'and', 'these', 'are', 'telegraph', 'way', 'in', 'advanc', 'errol', 'is', 'funni', 'as', 'the', 'stuffi', 'lord', 'ep', 'but', 'would', 'have', 'prefer', 'lot', 'more', 'wit', 'and', 'much', 'less', 'repetit', 'br', 'br'] \n",
      "\n",
      "written by someone who has been there you can tell but only if youve been there excellent performances by meryl streep of course renee zellweger and william hurtbr br many people have said that it is about a dysfunctional family i think every family is dysfunctional when they are facing this kind of torment to not be dysfunctional would be dysfunctional you are losing your family as you know it can anything be worse people need to see this movie so when they are faced with this nightmare maybe they will change how they do it maybe they will see that the father is denying himself valuable time hell never get a chance at again maybe they will realize how hard it is to die or to watch someone you love die they didnt miss much of the nightmare its hard to forget \n",
      "\n",
      "Stemmed text:\n",
      "['written', 'by', 'someon', 'who', 'has', 'been', 'there', 'you', 'can', 'tell', 'but', 'onli', 'if', 'youv', 'been', 'there', 'excel', 'perform', 'by', 'meryl', 'streep', 'of', 'cours', 'rene', 'zellweg', 'and', 'william', 'hurtbr', 'br', 'mani', 'peopl', 'have', 'said', 'that', 'it', 'is', 'about', 'dysfunct', 'famili', 'think', 'everi', 'famili', 'is', 'dysfunct', 'when', 'they', 'are', 'face', 'this', 'kind', 'of', 'torment', 'to', 'not', 'be', 'dysfunct', 'would', 'be', 'dysfunct', 'you', 'are', 'lose', 'your', 'famili', 'as', 'you', 'know', 'it', 'can', 'anyth', 'be', 'wors', 'peopl', 'need', 'to', 'see', 'this', 'movi', 'so', 'when', 'they', 'are', 'face', 'with', 'this', 'nightmar', 'mayb', 'they', 'will', 'chang', 'how', 'they', 'do', 'it', 'mayb', 'they', 'will', 'see', 'that', 'the', 'father', 'is', 'deni', 'himself', 'valuabl', 'time', 'hell', 'never', 'get', 'chanc', 'at', 'again', 'mayb', 'they', 'will', 'realiz', 'how', 'hard', 'it', 'is', 'to', 'die', 'or', 'to', 'watch', 'someon', 'you', 'love', 'die', 'they', 'didnt', 'miss', 'much', 'of', 'the', 'nightmar', 'it', 'hard', 'to', 'forget'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatizer errors:\")\n",
    "wrong_prediction_sample = test[test['label'] != predictions_lemmatizer][\"text\"].sample(2, random_state=SEED)\n",
    "for review in wrong_prediction_sample:\n",
    "    print(review, \"\\n\")\n",
    "    print(\"Lemmatized text:\")\n",
    "    print(tokenize(review), \"\\n\")\n",
    "\n",
    "print(\"==========================================================================\")\n",
    "print(\"==========================================================================\\n\")\n",
    "\n",
    "print(\"Stemmer errors:\")\n",
    "wrong_prediction_sample = test[test['label'] != predictions_stemmer][\"text\"].sample(2, random_state=SEED)\n",
    "for review in wrong_prediction_sample:\n",
    "    print(review, \"\\n\")\n",
    "    print(\"Stemmed text:\")\n",
    "    print(list(stemmed_words(review)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3a8af",
   "metadata": {},
   "source": [
    "We can see that both lemmatization and stemming make mistakes and creates word which surely contributes to the error in the classification of the above examples.\n",
    "\n",
    "In the first example review, we find words with positive connotation such as: talent, cuteness, good and beautifull. But we also find words with negative connotation: underused, godawful, frighteningly terrible. Having both positive and negative connotated words makes the classification harder, this may explain the classification error."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "20c84f793d7c421ab279e43d0dc90979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_caf8af8b5a7245feb18b49ed18b89fc7",
      "placeholder": "​",
      "style": "IPY_MODEL_3d291fe4a8844a9f9ffd249d02381a0a",
      "value": " 3/3 [00:00&lt;00:00, 51.55it/s]"
     }
    },
    "36dae29f9bdf4fd19112ae447f108d46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d291fe4a8844a9f9ffd249d02381a0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "404804524c8244babb9ec733793b4862": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_beeb81c07ee0433da0ff5be34562c12f",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a5bc64e1db24a71b7c38f5866355f51",
      "value": 3
     }
    },
    "4a5bc64e1db24a71b7c38f5866355f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ad766b38ffe4007b334e0f152a03584": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f802aac9442d460987498a692a417875",
       "IPY_MODEL_404804524c8244babb9ec733793b4862",
       "IPY_MODEL_20c84f793d7c421ab279e43d0dc90979"
      ],
      "layout": "IPY_MODEL_5ecc4acd0ae742d08a2cd60961611460"
     }
    },
    "4ffaae2a32f9405fb72dcb60bbec0796": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ecc4acd0ae742d08a2cd60961611460": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "beeb81c07ee0433da0ff5be34562c12f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caf8af8b5a7245feb18b49ed18b89fc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f802aac9442d460987498a692a417875": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ffaae2a32f9405fb72dcb60bbec0796",
      "placeholder": "​",
      "style": "IPY_MODEL_36dae29f9bdf4fd19112ae447f108d46",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
